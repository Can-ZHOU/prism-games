\documentclass{llncs}

\usepackage{llncsdoc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amssymb,stmaryrd}
\usepackage{soul}
\usepackage{color}
\usepackage{tikz}
%\usepackage{subfigure}
\usepackage{subfig}
\usepackage{hyperref}

\newcommand{\LD}{\langle}
\newcommand{\RD}{\rangle}
\def\Rset{\mathbb{R}}
\def\Rsetgeq{\mathbb{R}_{\geq 0}}
\def\Rsetge{\mathbb{R}_{> 0}}

\newcommand{\aistis}[1]{\marginpar{\footnotesize \color{red} {\bf A:} \textsf{#1}}}
\newcommand{\taolue}[1]{\marginpar{\footnotesize \color{red} {\bf TL:} \textsf{#1}}}
\newcommand{\dave}[1]{\marginpar{\footnotesize \color{red} {\bf D:} \textsf{#1}}}
\newcommand{\comment}[1]{\marginpar{\footnotesize \color{red} \textsf{#1}}}

\newcommand{\prismcomment}[1]{\mbox{\em #1}}
\newcommand{\prismkeyword}[1]{\mathtt{#1}}
\newcommand{\prismident}[1]{\mathit{#1}}
\newcommand{\prismtab}{\hspace*{0.5cm}}

\begin{document}

\title{Verifying Team Formation Protocols in PRISM\thanks{This work is supported by
the ERC Advanced Grant VERIWARE.}}
\author{}
\institute{
  Computing Laboratory, University of Oxford\\
  Wolfson Building, Parks Road, Oxford, OX1 3QD, UK
}
\email{}

\maketitle

\begin{abstract}
Multi-agent systems are becoming an increasingly important software paradigm. One of the key advantages of multi-agent systems is that agents can cooperate to achieve certain goals, that requires efficient collaboration protocols of which team formation is a typical example. In this paper we illustrate how probabilistic model checking,
a technique for formal verification of probabilistic systems,
can be applied to the analysis, design and verification of such protocols.
\dave{re-add mention of prism}
To start with, we take a team formation protocol and analyse its performance by modelling it
as a Discrete-Time Markov Chain. Then ... (mention extending the protocol) by modelling as a Markov Decision Process we show %what is
the best performance %that can be achieved by the team joining algorithm run by each agent in the protocol.
of running the team joining algorithm individually by each agent in the protocol. The performance includes the agent organization one and each agent's local one. Finally we extend such analysis to the competitive coalitional setting using Stochastic Two-Player Games where agents are divided into cooperative and Byzantine ones.
\end{abstract}

\section{Introduction}

\comment{Add some comments about complexity}
%
Multi-agent systems have become an important software paradigm. One of the key advantages of multi-agent systems is that agents can cooperate to achieve certain goals, that requires efficient collaboration protocols of which \emph{team formation} is a typical example. In this paper, we focus on a typical distributed team formation protocol, introduced in \cite{gaston2005agent}. There, the authors used it to analyse the team performance in dynamic networks.
The protocol %was studied further in \cite{glinton2008agent}, and
has also been applied to coalition formation for data fusion in sensor networks \cite{glinton2008agent}.

The basic setting for the protocol consists of an \emph{agent organization}, i.e., a network of interconnected agents which have certain resources. These agents attempt to form teams in order to accomplish tasks which are generated periodically and globally advertised to the agent organization. The topology of the network restricts the set of possible agent teams -- for an agent to be on a team, the agent must have a connection with at least one other agent in that team. Tasks are generic in that they only require a team of agents with the necessary resources to accomplish the specific task. As in \cite{gaston2005agent}, we do not consider the solution process, but only the team formation.

\emph{Formal verification} is an approach to check the correctness of a system
using rigorous, mathematical reasoning.
Fully automated verification techniques such as \emph{model checking}
have proved to be widely applicable,
including to multi-agent systems \cite{lomuscio2006mcmas}.
In this paper, the systems that we study exhibit \emph{probabilistic} behaviour,
for example because the likelihood of certain tasks being generated follows a known distribution,
or because the algorithms employed by individual agents are randomised.
Thus, we use \emph{probabilistic model checking}, an automated technique for
the formal verification of systems that exhibit stochastic behavior.

%It can, for example,
%calculate the likelihood of the occurrence of certain events
%during the execution of a system, and can be useful to establish performance
%measures. It is based on the construction and analysis of a
%mathematical model of the system, usually from a specification
%in some high-level description language. This model
%generally comprises a set of states, representing all the possible
%configurations of the system, the transitions that can
%occur between these states, and information about when and
%with what probability each transition will occur, and a probabilistic model checker applies algorithmic techniques to analyze the state space and calculate performance measures.

Probabilistic model checking is based
\dave{not just correctness, performance etc. too}
on the construction of a probabilistic model from a precise,
high-level description of a system's behaviour. A quantitative
analysis of this model is then performed, by applying a
combination of exhaustive search techniques and numerical
solution methods. In this paper we investigate how probabilistic model checking techniques can be applied for analysis, design and verification of team formation protocols. The majority of existing analyses of such protocols are based on discrete-event simulation (see, e.g. \cite{gaston2005agent}). In contrast, probabilistic model checking provides both an exhaustive search of all possible behaviours of the system, including best- and worst-case scenarios, and exact, rather than approximate, quantitative results. Of course, a trade-off inevitably exists. Simulation-based approaches are scalable to much larger and more complex models, at the expense of exhaustiveness and numerical accuracy. The intention of this work is not to show that model checking is ``better" than simulation-based approaches, but rather to highlight that model checking can be used in conjunction with simulation to provide additional insights into a system. In particular, for distributed protocols (like the one in this paper) whose behaviour is notoriously difficult to understand precisely, %inherently,
formally analysing a small system can be of great help to obtain a better understanding of large ones.

%\comment{TL: the above paragraph is toooooooooooooooooooooooooooooooooooo long?}

\comment{TL: mention the online v.s. offline version?}

\paragraph{Contributions.} We apply probabilistic model checking techniques to analyse a team
formation protocol \cite{gaston2005agent} using the PRISM model checker \cite{KNP11} with respect to
a given agent organization. Firstly, we model the original version of the protocol using Discrete Time Markov Chains (DTMCs), where the behaviour of each agent is described entirely in a probabilistic (deterministic) way. Then we extend the original algorithm  by allowing agents to make decisions nondeterministically instead of purely probabilistically when forming teams; such systems are naturally modeled by Markov Decision Processes (MDPs). By analysing the MDP we obtain the best- and worst-case performance of agent organisations, as well as the one of a single agent. However, MDPs only allow fully collaborative behaviours, whereas in many scenarios it is crucial to address hostile behaviour of some agents in the organisation. To cater for this we use Stochastic Two-Player Games (STPGs) as a model for the system containing two groups of agents -- collaborative and Byzantine, which try to maximise or minimise the performance of the organisation respectively. In all of these three models the performance of an agent organisation or individual agents is analysed by model checking PCTL formulae.

Our experiments illustrate several aspects of agent organisation analysis. As a typical case, we choose four network topologies each consisting of five agents, i.e., fully connected, ring, start, and a network having one isolated node. For each one, we compute the expected performance of the organisation and find organisation-optimal resource allocation among agents. Then we show using MDP model checking what is the best performance that can be achieved by this organisation in two settings, \emph{offline} and \emph{online}, depending on whether the tasks are generated respectively \emph{before} and \emph{after} teams have formed. Then we take the model to STPG setting to obtain the optimal coalitions of different sizes and evaluate their performance.

%\comment{resource v.s. skill?}
%
%\comment{add several advantages of our approach - base on results. }

Comparing to the traditional simulation-based analysis, we think one of the greatest advantages of our work lies in that we can actually synthesize the best strategies for agents to achieve the optimal performances. Note that in \cite{gaston2005agent}, this was posed as an open problem.\footnote{We quote here: ``the problem of developing or learning effective team initialing and team joining policies is also important, and is included in our on-going and future work".} Our work on extensions of the \textsc{TeamJoin} algorithm and the analysis of the associated MDP and STPG models can be seen as a partial solution to this problem. Moreover, our experience indicates that by simply adapting the PRISM code, one could analyse the behaviour of different agent organizations very easily, which can be considered as another advantage.


\paragraph{Related work.}
As one of the greatest advantages of the agent based computing, cooperative behaviour has been studied from many different angles over the years. Coalitional games traditionally have been analysed from game-theoretic perspective \cite{osborne1994course}, but in recent years has attracted a lot of attention from researchers in artificial intelligence, especially in cooperative task completion \cite{shehory1998methods}. Several approaches for team formation and collaborative task solving are considered including team formation under uncertainty using simple heuristic rules \cite{kraus2003coalition}, reinforcement learning techniques \cite{abdallah2004organization}, and methods using distributed graph algorithms \cite{manisterski2006forming}. To reason formally about cooperative games several logics (e.g., Coalitional Game Logic \cite{agotnes2009reasoning}, Strategy Logic \cite{chatterjee2007strategy}), and other formalisms (e.g., Cooperative Boolean Games \cite{dunne2008cooperative}) have been introduced and used to analyse coalitional behaviours \cite{bonzon2007efficient}. Model checking has been used to analyse some protocols related to multi-agent system, for instance, clock synchronisation in wireless sensor networks using Uppaal \cite{heidarian2009analysis}, Gossip \cite{KNP08d} and Byzantine agreement protocols \cite{KN02} using PRISM \cite{KNP11}. MCMAS model checker has been applied to verification of knowledge-based properties in multi-agent systems and classic problems like dining cryptographers \cite{lomuscio2006mcmas}.

\comment{TL: stress on the novelty of the work...}

%PRISM has also been used to model and analyse
%a number of different wireless protocols, for example, the
%IEEE 802.11 backoff mechanism [14] and device discovery
%in Bluetooth [5]. See the PRISM case study repository [16]
%for more details and further examples.

\paragraph{Structure.}


\section{Preliminaries}

%In this paper, we shall us discrete-time
%Markov chains (DTMCs), Markov decision processes (MDPs), and Stochastic Two-Player Games (STPGs) as the modeling %formalism. %where time is modelled as
%%discrete steps and the probability of making each transition
%%is given by a discrete probability distribution.
%Below we give a short introduction on these models.

%----------------------------------------------------------------------------------------------------------------

\subsection{Probabilistic Models}

We begin with a brief introduction to the three different types of probabilistic models
that we will use in this paper.

%\vskip5pt
\emph{Discrete-time Markov chains} (DTMCs) are the simplest of these models.
A DTMC $(S,\mathbf{P})$ is defined by a set of states $S$ and a probability transition matrix
$\mathbf{P} : S\times S \rightarrow [0, 1]$, where $\sum_{s'\in S} \mathbf{P}(s, s') = 1$ for all $s \in  S$.
This gives the probability $\mathbf{P}(s, s')$ that a transition will take place from state $s$ to state $s'$.

%\vskip5pt
\emph{Markov decision processes} (MDPs) extend DTMCs by incorporating \emph{nondeterministic choice}
in addition to probabilistic behaviour.
An MDP $(S,Act,Steps)$ comprises a set of actions $Act$
and a (partial) probabilistic transition function $Steps:S\times Act\rightarrow Dist(S)$,
which maps state-action pairs to probability distributions over the state space $S$.
In each state $s\in S$, one or more distinct actions can be taken and, assuming that action $a\in Act$ is chosen,
the distribution $Steps(s,a)$ gives the probability of making a transition to each state.

%Markov Decision Processes (MDP) allow accurate modelling of systems which exhibit
%both probabilistic and nondeterministic behavior. Formally, a Markov decision process (MDP) is a tuple $(S, s_0, Act, %Steps)$ where $S$ is a finite set of states,
%$s_0$ is the initial state, $Act$ is a set of actions
%and $Steps\subseteq S \times Act \times Dist(S)$ is a probabilistic transition relation.
%An MDP transition $s\stackrel{a,\mu}s'$ is made from a state $s\in S$ first by nondeterministically
%selecting an action-distribution pair $(a, \mu)$ such that $(s, a, \mu) \in
%steps$, and second by making a probabilistic choice of the target state $s'$ according to the distribution
%$\mu$, such that $\mu(s') > 0$. A path represents a particular

%An \emph{adversary} represents a particular resolution
%of nondeterminism only. Formally, an adversary of a MDP is a function $A$
%mapping every finite path $\pi$ to a pair $(a, \mu)$ such that $(last(\pi), a, \mu)\in Steps$.
%The evolution of the MDP according to a particular adversary $A$ is a
%measurable set of infinite paths associated with $A$, which can classically be
%provided with a probability measure.

%\vskip5pt
\emph{Stochastic two-player games} (STPGs) generalise MDPs by allowing the nondeterministic choices
in the model to be resolved by two distinct players.
An STPG is a tuple $(S,(S_1,S_2),Act,Steps)$
where the set of states $S$ is partitioned into two disjoint subsets $S_1$ and $S_2$.
As for MDPs, $Steps:S\times Act\rightarrow Dist(S)$ is a function mapping
state-action pairs to distributions over states.
This is a \emph{turn-based} game. In each state $s$ of the game,
either player 1 or player 2 selects an action $a\in Act$,
depending on whether $s$ is in set $S_1$ or $S_2$.

For all three classes of models, we also consider \emph{rewards},
which can be used to capture a variety of additional quantitative measures
(e.g. ...)
For consistency, we assume a simple state-based scheme
i.e. a \emph{reward function} $\rho:S\rightarrow\Rsetgeq$.
\dave{might move rewards to next sect}

%----------------------------------------------------------------------------------------------------------------

\subsection{Probabilistic Model Checking \& PRISM}

Probabilistic model checking involves the construction and analysis of a probabilistic model.
Typically, a high-level description language is used to model a system
(here, we will use the PRISM \cite{KNP11} modelling language).
Then, one or more quantitative properties are formally specified and analysed on the model.
Usually, probabilistic extensions of temporal logic are used to formalise properties.
In this paper, we use the logic PCTL \cite{HJ94},
extended to include reward-based properties \cite{KNP07a},
which can be used to express properties of both DTMCs and MDPs.
We will also generalise the logic to capture properties of stochastic games.

PCTL extends the well known temporal logic CTL, with a probabilistic ($\tt P$) operator.
Informally, this places bounds on the probability of the occurrence of certain events in the model.
We will illustrate the use of PCTL using some simple examples,
referring the reader to \cite{HJ94,KNP07a} for precise details of the syntax and semantics.
For a DTMC, typical PCTL properties would be:
\begin{itemize}
\item ${\tt P}_{<0.01}[\,\Diamond\,\mathit{fail}\,]$ -
``the probability of a failure occurring is less than 0.01''
\item ${\tt P}_{\geq 0.95}[\,\Diamond^{\leq 60}\,\mathit{end}\,]$ -
``the probability of the protocol completing with 60 time units is at least 0.95''.
\end{itemize}
In fact, in practice, we often use a \emph{quantitative} variant of the ${\tt P}$ operator,
denoted ${\tt P}_{=?}$, which returns the actual probability of an event's occurrence, e.g.:
\begin{itemize}
\item ${\tt P}_{=?}[\,\Diamond\,\mathit{fail}\,]$ -
``what is the probability of a failure occurring?''
\end{itemize}
Whereas in a DTMC it is relatively straightforward to define the probability of the occurrence
of an event (such as $\Diamond\,\mathit{fail}$ or $\Diamond^{\leq 60}\,\mathit{end}$ in the examples above),
for MDPs, we must also take account of the nondeterminism in the model.
The standard approach is to use the notion of
\emph{strategies} (also referred to as \emph{policies}, \emph{schedulers}, etc.).
A strategy resolves nondeterminism in an MDP (i.e. chooses an action in a state),
based on its execution history so far.
For a specific strategy, we \emph{can} define the probability of an event.
Thus, probabilistic model checking focuses on a best- or worst-case analysis,
quantifying over all possible strategies.
We will continue to focus on quantitative properties,
which now ask for the \emph{minimum} or \emph{maximum} probability of an event occurring, e.g.:
\begin{itemize}
\item ${\tt P}_{\max=?}[\,\Diamond\,\mathit{fail}\,]$ -
``what is the maximum probability of a failure occurring?''
\end{itemize}
For a stochastic game, the same approach generalises naturally,
but we require strategies for both players.
Usually, we assume that the two players have opposing objectives,
for example player 1 aims to minimise the probability of $\Diamond\,\mathit{fail}$
and player 2 tries to maximise it.
Extending the notation from above, we write:
\begin{itemize}
\item ${\tt P}_{\min,\max=?}[\,\Diamond\,\mathit{fail}\,]$ -
``what is the minimum probability of failure that player 1 can guarantee,
assuming that player 2 tries to maximise it?''
\end{itemize}
\dave{could put formal details in an appendix, if that helps}

For this simple class of (reachability) properties,
these values are well defined and \dave{bla bla bla}

Finally, we also include reward-based properties.
More precisely, we consider the \emph{expected total reward} accumulated until some target set of states is reached.
Consider a DTMC and reward structure $\mathit{time}$... and label (?) $\mathit{end}$...

\begin{itemize}
\item ${\tt R}^{\mathit{time}}_{=?}[\,\Diamond\,\mathit{end}\,]$ -
``what is the expected time for the algorithm to complete?''
\end{itemize}
In exactly, the same style as described above, these queries generalise to MDPs and stochastic games, e.g.:
\begin{itemize}\addtolength{\itemsep}{2pt}
\item ${\tt R}^{\mathit{time}}_{\max=?}[\,\Diamond\,\mathit{end}\,]$ -
``what is the maximum expected algorithm completion time?''
\item ${\tt R}^{\mathit{time}}_{\min,\max=?}[\,\Diamond\,\mathit{end}\,]$ -
``what is the minimum expected time for algorithm completion
that player 1 can guarantee, assuming player 2 tries to maximise it?''
\end{itemize}

\dave{mention infinite case, non prob 1}

\dave{mention alg.s, complexity}

\emph{PRISM} \cite{KNP11} is a probabilistic model checking tool.
It supports several different types of models, including DTMCs and MDPs
(it also supports continuous-time Markov chains and probabilistic timed automata).
For this work, we have built a prototype extension of PRISM that adds support for two-player stochastic games.

...

These models are
described in a high-level language, a variant of reactive modules [2] based on guarded
commands. This is described in more detail in Section....

PRISM accepts specifications in either PCTL, or CSL logic depending on the model
type. The model checker then analyses the model and checks if the property holds in each
state.
The reader is invited to consult the PRISM
web site \cite{} for detailed information and corresponding
publications about all of these.

\iffalse
The basic components of the language are modules and variables. A
system is constructed as a number of modules which can interact with each other.
A module contains a number of variables which express the state of the module,
and its behavior is given by a set of guarded commands of the form:

[] $\langle$guard$>$ \! $\langle$command$>$;

The guard is a predicate over the variables of the system and the command describes
a transition which the module can make if the guard is true (using primed variables
to denote the next values of variables). If a transition is probabilistic, then the
command is specified as:

$\langle prob \rangle$ : $\langle$ command $\rangle$ + ...+ $\langle$ prob$\rangle$ : <command>

\fi


\section{Definitions and Algorithms}

The purpose of this section is to provide definitions of terminology
used throughout this paper and then present the algorithms which will be analysed.

\subsection{Definitions}

We now introduce definitions of \emph{agent organisations}, \emph{tasks}, \emph{teams}, and formulae for computing \emph{rewards} to measure the performance of both individual agents and agent teams.

\begin{definition}[Agent Organisation]
An \emph{agent organisation} is a tuple $O=\langle A, N, R, R_A  \rangle$ where
\noindent
\begin{itemize}
\item $A= \{ a_1,a_2,...,a_n \} $ is a set of agents,
\item $N=\{\{a_i,a_j \}\ :$ ``$a_i$ and $a_j$ are neighbours'' $\}$ is a neighbourhood relation,
\item $R=\{r_1,r_2,\dots,r_k\}$ is a set of resource types, and
\item $R_A=\{R_{a_1}, R_{a_2}, \dots, R_{a_n}\}$ is a set of agent resources where $r_j \in R_{a_i} $ $\iff$ ``agent $a_i$ has a resource $r_j$''.
\end{itemize}
\end{definition}

\begin{definition}[Task]
A \emph{task} $T_i=\{r_i\ :\ $``$r_i$ is required by the task $i$''$\}$ is a set of resources that are required to accomplish $T_i$. By $T=\{T_1, T_2,\dots, T_t\}$ we denote a collection of tasks.
\end{definition}

\begin{definition}[Team]
A \emph{team} of agents is denoted by $M_i=\{a_j\ :\ $``$a_j$ is a member of team $i$''$\}$, and the set of all teams is $M=\{M_1, M_2,\dots , M_m\}$. By $\bar{M} = \bigcup_{1\leq i\leq m} M_i$, we denote the set of all agents that are committed to some team. For $1\leq i\leq m$, $R_{M_i}=\bigcup_{a \in M_i}R_{a}$ is the set of resources the team $M_i$ has. The team $M_i$ is said to be able to accomplish the task $T_j$ iff $T_j \subseteq R_{M_i}$.
\end{definition}

\begin{definition}[Rewards]
For agent $a$, we define two types of \emph{reward}:
\begin{itemize}
    \item Type $W_1$, which rewards the \emph{agent} with 1 point if it is in the team which was able to complete its task after team formation is over; and 0 otherwise. Formally,
    \begin{equation} \label{eq:w1agent}
W_1(a) = \left\{
  \begin{array}{l l}
    1 & \quad \text{if $\exists M_i . a \in M_i \wedge T_i \subseteq R_{M_i}$,}\\
    0 & \quad \text{otherwise,}\\
  \end{array} \right.
\end{equation}

    \item Type $W_2$, which rewards 1 point to the \emph{team} which was able to complete its task, and 0 otherwise. The reward is shared equally between team members.
\begin{equation}
W_2(a) = \left\{
  \begin{array}{l l}
    \frac{1}{|M_i|} & \quad \text{if $\exists M_i . a \in M_i \wedge T_i \subseteq R_{M_i}$,}\\
    0 & \quad \text{otherwise.}\\
  \end{array} \right.
\label{eq:w2agent}
\end{equation}
\end{itemize}


For a set of agents $A$, the rewards are defined accordingly as the total reward achieved by its members, i.e.,
\begin{equation}
 W_1(A) = \sum_{a \in A} W_1(a),
\label{eq:w1organisation}
\end{equation}
\begin{equation}
 W_2(A) = \sum_{a \in A} W_2(a).
\label{eq:w2organisation}
\end{equation}
\end{definition}


\subsection{Algorithms}

In this section we provide pseudocode descriptions of algorithms which we later analyse. During the team formation process, each agent performs as follows:
when it is not committed to any team (meaning that it is available and not assigned to any task), it considers each task in a random order. If a task currently
has no other agents committed to it, it can choose to initiate a team, and does so with a probability equal to the ratio between neighbours that are not committed to any team and total number of neighbours. Formally, the probability that an agent $a$ initiates a team is
\[
IP_a = \frac{|\{ a' \in A\ :\ \{a, a'\} \in N \wedge a' \notin \bar{M}  \}|}{|\{ a' \in A\ :\ \{a, a'\} \in N \}|}.
\label{eq:init_prob}
\]
For team joining, if an agent is eligible for a team, it always joins the team. Note that only uncommitted agents can commit to
a new or partially filled task, and committed agents can not decommit from a given task.
%
%can be in one of three states: \textsc{uncommitted}, \textsc{committed}, or \textsc{active}. An agent in the \textsc{uncommitted}
%state is available and not assigned to any task. An agent in the \textsc{committed} state has selected a task, but the full team to work in the task has not yet formed. Finally an agent in the  \textsc{active} state is a member of a team that has fulfilled all of the skill requirements for a task and is actively working on that task. Upon task completion, agents in the active state return to the uncommitted state.
%The core of the protocol is the \textsc{JoinTeam} algorithm, which is used for each agent in the \textsc{uncommitted}
%state to decide which teams to initiate or join. (The algorithm combines team initiation and team joining.)

%\begin{definition}[Initiation probability]
%The team \emph{initiation probability} for agent $a$ used in Algorithm \ref{alg:join_team_org} is the ratio between neighbours that are not committed to any team and total number of neighbours:
%\begin{equation}
%IP_a = \frac{|\{ a' \in A\ :\ \{a, a'\} \in N \wedge a' \notin \bar{M}  \}|}{|\{ a' \in A\ :\ \{a, a'\} \in N \}|}.
%\label{eq:init_prob}
%\end{equation}
%\end{definition}


In Algorithm \ref{alg:join_team_org} we reproduce the pseudocode of the \textsc{JoinTeam} algorithm introduced in \cite{gaston2005agent}. (It combines team initiation and team joining.) %\textsc{JoinTeam} algorithm is executed separately on each agent whenever it is called.
This algorithm will be modelled and analysed as a DTMC, as we shall see in Section \ref{}.

\begin{algorithm}[H]
\caption{Team joining algorithm \cite{gaston2005agent} (probabilistic and deterministic)}
\label{alg:join_team_org}
\begin{scriptsize}
\begin{algorithmic}
\Procedure{JoinTeam}{$a$, $T$, $M$}
  \ForAll {$T_i \in T$ in random order}
    \If {$a \notin \bar{M}$} \Comment{agent is not committed}
      \If {$|M_i|=0$} \Comment{team for task i is empty}
	\If {$R_a \cap T_i \neq \emptyset$} \Comment{agent has skill (is true if called from \textsc{Online})}
	  \State with probability $IP_a$: $M_i \leftarrow M_i \cup \{a\}$ \Comment{initiate a team (see equation \ref{eq:init_prob})}
	\EndIf
      \ElsIf {$\exists \{a,a'\} \in N . a' \in M_i$} \Comment{there is neighbour in team for task i}
	\If {$R_a \cap T_i \setminus R_{M_i} \neq \emptyset$} \Comment{agent has a missing resource}
	  \State $M_i \leftarrow M_i \cup \{a\}$ \Comment{join team}
	\EndIf
      \EndIf
    \EndIf
  \EndFor
\EndProcedure
\end{algorithmic}
\end{scriptsize}
\end{algorithm}
%
%\comment{TL: the problem of developing or learning effective team initialing and team joining policies is also important ...}

To tackle the problem of finding the best team initialing and team joining strategy, we modify the original \textsc{JoinTeam} algorithm by
allowing agents to make decisions regarding what actions to take, instead of picking one randomly. Technically, the changes are as follows, which
are highlighted in Algorithm \ref{alg:join_team_nondet}.
\begin{itemize}
 \item Remove restriction of considering tasks in random order allowing agent to consider arbitrary task;
 \item Replace probabilistic choice to initiate the team by nondeterministic one; and
 \item Allow agent not to join a team even if it has the resource and neighbour in that team.
\end{itemize}

This algorithm allows for the analysis of best-case performance that can be achieved by the protocol and also to analyse the agent organisations with \emph{hostile} agents which try to reduce organisation's performance. It will be modelled and analysed as an MDP and a STPG respectively, as we shall see in Section \ref{}.

\begin{algorithm}[H]
\caption{Team joining algorithm (non-deterministic)}
\label{alg:join_team_nondet}
\begin{scriptsize}
\begin{algorithmic}
\Procedure{JoinTeam}{$a$, $T$, $M$}
  \ForAll {$T_i \in T$ in \hl{arbitrary} order}
    \If {$a \notin \bar{M}$} \Comment{agent is not committed}
      \If {$|M_i|=0$} \Comment{team for task i is empty}
	\If {$R_a \cap T_i \neq \emptyset$} \Comment{agent has skill (is true if called from \textsc{Online})}
	    \State \hl{$M_i \leftarrow M_i \cup \{a\}$  \textbf{or} $M_i \leftarrow M_i$} \Comment{initiate a team or do nothing}
	\EndIf
      \ElsIf {$\exists \{a,a'\} \in N . a' \in M_i$} \Comment{there is neighbour in team for task i}
	\If {$R_a \cap T_i \setminus R_{M_i} \neq \emptyset$} \Comment{agent has a missing resource}
	  \State \hl{$M_i \leftarrow M_i \cup \{a\}$ \textbf{or} $M_i \leftarrow M_i$} \Comment{join a team or do nothing}
	\EndIf
      \EndIf
    \EndIf
  \EndFor
\EndProcedure
\end{algorithmic}
\end{scriptsize}
\end{algorithm}

We observe that there are two natural ways to call \textsc{JoinTeam}: \textsc{Offline} procedure first initiates the set of tasks and then sequentially calls \textsc{JoinTeam} procedures of every agent in random order, which is described in Algorithm \ref{alg:join_team_org}; In contrast, \textsc{Online} routine calls \textsc{JoinTeam} procedures for agents \emph{before} selecting the tasks. We investigate both the offline and the online version of the algorithms because it provides a nice comparison between optimisation against specific tasks (offline version), and distribution of tasks (online version). As we will see in the results section for STPGs, whether offline and online version provides better performance depends on network topology.


\begin{algorithm}[H]
\caption{Offline and online versions of the algorithm}
\label{alg:main_process}
\begin{scriptsize}
\begin{algorithmic}
\Procedure{Offline}{$t$} \Comment{$t$ - number of tasks}
  \State $M = \{M_i = \emptyset\ :\ 1\leq i \leq t\}$ \Comment{initialise empty teams}
  \State $T = \{T_i\neq \emptyset\ :\ T_i \subseteq_{random} R,\ 1\leq i \leq t\}$ \Comment{initialise tasks at random}
  \ForAll {$a \in A$ in random order}
    \State \Call{JoinTeam}{$a$, $T$, $M$}
  \EndFor
  \State perform tasks and compute rewards
\EndProcedure
\Statex
\Procedure{Online}{$t$} \Comment{$t$ - number of tasks}
  \State $M = \{M_i = \emptyset\ :\ 1\leq i \leq t\}$ \Comment{initialise empty teams}
  \ForAll {$a \in A$ in random order}
    \State \Call{JoinTeam}{$a$, $T$, $M$}
  \EndFor
  \State $T = \{T_i\neq \emptyset\ :\ T_i \subseteq_{random} R,\ 1\leq i \leq t\}$ \Comment{initialise tasks at random}
  \State perform tasks and compute rewards
\EndProcedure
\end{algorithmic}
\end{scriptsize}
\end{algorithm}

\section{Models and Experimental Setup}

\subsection{PRISM Models}

PRISM model checker has been briefly described in the introduction. The purpose of this section is to explain how we modelled the algorithms from the previous section in PRISM. Due to size limitations we do not provide the source code of the models and properties used in this paper, instead, we made it all available online\footnote{Webpage for PRISM models}. In this section we use a toy example from Figure \ref{fig:prism_example} to illustrate the design concepts.

\begin{figure}
 \centering
{\scriptsize
\fbox{\begin{minipage}{6in}
\begin{tabbing}
\mbox{$\prismkeyword{module} \; \prismident{scheduler}$} \\
\mbox{$\prismtab\prismident{turn} \; : \; [1..3] \; \prismkeyword{init} \; 1;$} \\
\mbox{$\prismtab\prismident{num\_tasks} \; : \; [-1..2] \; \prismkeyword{init} \; $-1$;$} \\
\mbox{$\prismtab[] \; \prismident{num\_tasks}{=}$-1$ \; \rightarrow \; 0.5 \; : \; (\prismident{num\_tasks'}{=}1) \; + \; 0.5 \; : \; (\prismident{num\_tasks'}{=}2);$} \\
\mbox{$\prismtab[\prismident{go1}] \; \prismident{num\_tasks}{>}0 \; \wedge \; \prismident{turn}{=}1 \; \rightarrow \; (\prismident{turn'}{=}2);$} \\
\mbox{$\prismtab[\prismident{go2}] \; \prismident{num\_tasks}{>}0 \; \wedge \; \prismident{turn}{=}2 \; \rightarrow \; (\prismident{turn'}{=}3);$} \\
\mbox{$\prismtab[\prismident{do}] \; \prismident{num\_tasks}{>}0 \; \wedge \; \prismident{turn}{=}3 \; \rightarrow \; (\prismident{turn'}{=}1) \; \wedge \; (\prismident{num\_tasks'}{=}\prismident{num\_tasks}-1);$} \\
\mbox{$\prismkeyword{endmodule}$} \\
\mbox{$$} \\
\mbox{$\prismkeyword{module} \; \prismident{agent1}$} \\
\mbox{$\prismtab\prismident{team1} \; : \; [1..2] \; \prismkeyword{init} \; 1;$} \\
\mbox{$\prismtab[\prismident{go1}] \; \prismkeyword{true} \; \rightarrow \; (\prismident{team1'}{=}1);$} \\
\mbox{$\prismtab[\prismident{go1}] \; \prismkeyword{true} \; \rightarrow \; (\prismident{team1'}{=}2);$} \\
\mbox{$\prismkeyword{endmodule}$} \\
\mbox{$$} \\
\mbox{$\prismkeyword{module} \; \prismident{agent2} \; {=} \; \prismident{agent1} \; [\prismident{go1}{=}\prismident{go2},\prismident{team1}{=}\prismident{team2}]$} \\
\mbox{$\prismkeyword{endmodule}$} \\
\mbox{$$} \\
\mbox{$\prismkeyword{rewards} \; \mbox{``}\prismident{total}\mbox{''}$} \\
\mbox{$\prismtab\prismident{turn}{=}3 \; \wedge \; \prismident{team1}{\neq}\prismident{team2} \; : \; 0.3;\prismtab$} \\
\mbox{$\prismtab\prismident{turn}{=}3 \; \wedge \; \prismident{team1}{=}\prismident{team2} \; : \; 1.0;$} \\
\mbox{$\prismkeyword{endrewards}$}
\end{tabbing}
\end{minipage}}}
\caption{PRISM model of a two agent system. Explain meaning of guarded commands here.}
\label{fig:prism_example}
\end{figure}

The system modelled in this example consists of two agents and a scheduling module which generates the number of tasks to be performed (1 or 2 each with probability 0.5), then agents act in turn by choosing which team to join for each task. The reward structure ``total'' rewards 0.3 points for each task for which agents joined different teams and 1.0 points when agents cooperate. The team choice for an agent is nondeterministic (i.e. the underlying model will be either MDP or STPG), but it could be made probabilistic in a similar way the scheduler generates tasks (and thus become a DTMC).

The same principle has been applied to the models that we used for experiments. Each agent is modelled as a module with Algorithms \ref{alg:join_team_org} and \ref{alg:join_team_nondet} encoded as guarded commands. There is a scheduler module which has Algorithm \ref{alg:main_process} implemented as guarded commands. The reward structures are also described in a similar manner according to definitions provided in Equations \ref{eq:w1agent}-\ref{w2organisation}. 

From this high-level description of the algorithm PRISM then constructs the corresponding model: DTMC, MDP, or STPG (for example of the model in Figure \ref{fig:prism_example} see Appendix \ref{app:transitions}). For DTMCs and MDPs each state is encoded by values of the variables, there are additional states for synchronised actions. For STPGs in addition states are labelled with Player 1 and Player 2. For this to be done module names and synchronised actions have to be accordingly partitioned into two disjoint sets.

\subsection{Experimental Setup}
\label{subsec:exp_set}
For our experiments we mainly consider organisations consisting of five agents which are organised in four networks: fully connected, ring, start, and a network having one isolated node. Each agent is assigned one resource, and there are three different resources available. For each network we find the optimal resource allocation with respect to task generation described\footnote{We have chosen this way for allocating resources so that we could easily show how the performance can be improved by changing the strategy while keeping actions unchanged (i.e. compare DTMC and MDP models).} below using DTMC model checking (see section \ref{sec:results}). These organisations are then fixed and used in all experiments.

\subsubsection{Agent Organisations.}
We run experiments with a set of five agents $A= \{ a_1,a_2,a_3,a_4,a_5 \} $ and a set of  three resources $R=\{r_1,r_2,r_3\}$ arranged in four different agent organisations (see figure \ref{fig:network_configurations} for graphical representations of the organisations):
%\begin{itemize}
% \item Fully connected (figure \ref{subfig:fully_connected}) $O_{fc}=\langle A, N_{fc}, R, R_{A_{fc}}  \rangle$ where
%    \begin{itemize}
%    \item $N_{fc}=\{\{a_i,a_j \}\ : 1 \le i,j \le 5, i\neq j \}$,
%    \item $R_{A_{fc}}=\{R_{a_1}, R_{a_2}, R_{a_3}, R_{a_4}, R_{a_5}\}$ where $R_{a_1}=\{r_1\}$, $R_{a_2}=\{r_1\}$, $R_{a_3}=\{r_2\}$, $R_{a_4}=\{r_2\}$, and $R_{a_5}=\{r_3\}$.
%    \end{itemize}
% \item Ring (figure \ref{subfig:ring}) $O_r=\langle A, N_r, R, R_{A_r}  \rangle$ where
%    \begin{itemize}
%    \item $N_r=\{\{a_i,a_j \}\ : 1 \le i,j \le 5, i\neq j, i\le 5 \implies j=i+1, i=5 \implies j=1 \}$,
%    \item $R_{A_r}=\{R_{a_1}, R_{a_2}, R_{a_3}, R_{a_4}, R_{a_5}\}$ where $R_{a_1}=\{r_1\}$, $R_{a_2}=\{r_2\}$, $R_{a_3}=\{r_1\}$, $R_{a_4}=\{r_2\}$, and $R_{a_5}=\{r_3\}$.
%    \end{itemize}
% \item Star (figure \ref{subfig:star}) $O_s=\langle A, N_s, R, R_{A_s}  \rangle$ where
%    \begin{itemize}
%    \item $N_s=\{\{a_i,a_j \}\ : 1 \le i,j \le 5, i\neq j, i=1 \}$,
%    \item $R_{A_s}=\{R_{a_1}, R_{a_2}, R_{a_3}, R_{a_4}, R_{a_5}\}$ where $R_{a_1}=\{r_1\}$, $R_{a_2}=\{r_2\}$, $R_{a_3}=\{r_2\}$, $R_{a_4}=\{r_3\}$, and $R_{a_5}=\{r_3\}$.
%    \end{itemize}
% \item Isolated agent (figure \ref{subfig:isolated}) $O_{ia}=\langle A, N_{ia}, R, R_{A_{ia}}  \rangle$ where
%    \begin{itemize}
%    \item $N_{ia}=\{\{a_i,a_j \}\ : 1 \le i,j \le 5, i\neq j, (i=5 \vee j=5) \implies (i=4 \vee j=4) \}$,
%    \item $R_{A_{ia}}=\{R_{a_1}, R_{a_2}, R_{a_3}, R_{a_4}, R_{a_5}\}$ where $R_{a_1}=\{r_1\}$, $R_{a_2}=\{r_2\}$, $R_{a_3}=\{r_3\}$, $R_{a_4}=\{r_3\}$, and $R_{a_5}=\{r_2\}$.
%    \end{itemize}
%\end{itemize}


\subsubsection{Tasks.} \label{subsec:tasks} We fix seven different tasks that will be used in experiments $T=\{\{r_1\},\{r_2\},\{r_3\},\{r_1,r_2\},\{r_1,r_3\},\{r_2,r_3\},\{r_1,r_2,r_3\}\}$. When running the algorithm two tasks $T_1$ and $T_2$ are picked uniformly and independently at random (with replacement) and are advertised to the agent organisation. So there are total of 49 different combinations of $T_1$ and $T_2$ that can be generated.


\begin{figure}[H]
\centering
\subfloat[Fully connected]{
  \begin{tikzpicture}
      \tikzstyle{every node}=[draw,circle,fill=white,minimum size=5pt,
			      inner sep=0pt]
      \draw (0,0) 	 node (agent1) [label=90:$\LD a_1\ r_1 \RD$] {}
	  -- ++(-36:2.0cm)   node (agent2) [label=right:$\LD a_2\ r_1 \RD$] {}
	  -- ++(-108:2.0cm) node (agent3) [label=-30:$\LD a_3\ r_2 \RD$] {}
	  -- ++(-180:2.0cm) node (agent4) [label=210:$\LD a_4\ r_2 \RD$] {}
	  -- ++(108:2.0cm) node (agent5) [label=left:$\LD a_5\ r_3 \RD$] {}
	  -- (agent1);
      \draw (agent1) -- (agent3);
      \draw (agent1) -- (agent4);
      \draw (agent2) -- (agent4);
      \draw (agent2) -- (agent5);
      \draw (agent3) -- (agent5);
  \end{tikzpicture}
  \label{subfig:fully_connected}
}\ \ \
\subfloat[Ring]{
  \begin{tikzpicture}
      \tikzstyle{every node}=[draw,circle,fill=white,minimum size=5pt,
			      inner sep=0pt]
      \draw (0,0) 	 node (agent1) [label=90:$\LD a_1\ r_1 \RD$] {}
	  -- ++(-36:2.0cm)   node (agent2) [label=right:$\LD a_2\ r_2 \RD$] {}
	  -- ++(-108:2.0cm) node (agent3) [label=-30:$\LD a_3\ r_1 \RD$] {}
	  -- ++(-180:2.0cm) node (agent4) [label=210:$\LD a_4\ r_2 \RD$] {}
	  -- ++(108:2.0cm) node (agent5) [label=left:$\LD a_5\ r_3 \RD$] {}
	  -- (agent1);
  \end{tikzpicture}
  \label{subfig:ring}
}

\subfloat[Star]{
  \begin{tikzpicture}
      \tikzstyle{every node}=[draw,circle,fill=white,minimum size=5pt,
			      inner sep=0pt]
      \draw (0,0) 	 node (agent1) [label=90:$\LD a_1\ r_1 \RD$] {}
	  -- ++(-36:2.0cm)   node (agent2) [label=right:$\LD a_2\ r_2 \RD$] {}
	   ++(-108:2.0cm) node (agent3) [label=-30:$\LD a_3\ r_2 \RD$] {}
	   ++(-180:2.0cm) node (agent4) [label=210:$\LD a_4\ r_3 \RD$] {}
	   ++(108:2.0cm) node (agent5) [label=left:$\LD a_5\ r_3 \RD$] {}
	  -- (agent1);
      \draw (agent1) -- (agent3);
      \draw (agent1) -- (agent4);
  \end{tikzpicture}
  \label{subfig:star}
}\ \ \
\subfloat[Isolated]{
  \begin{tikzpicture}
      \tikzstyle{every node}=[draw,circle,fill=white,minimum size=5pt,
			      inner sep=0pt]
      \draw (0,0) 	 node (agent1) [label=90:$\LD a_1\ r_1 \RD$] {}
	  -- ++(-36:2.0cm)   node (agent2) [label=right:$\LD a_2\ r_2 \RD$] {}
	  -- ++(-108:2.0cm) node (agent3) [label=-30:$\LD a_3\ r_3 \RD$] {}
	  -- ++(-180:2.0cm) node (agent4) [label=210:$\LD a_4\ r_3 \RD$] {}
	  -- ++(108:2.0cm) node (agent5) [label=left:$\LD a_5\ r_2 \RD$] {};
      \draw (agent1) -- (agent3);
      \draw (agent1) -- (agent4);
      \draw (agent2) -- (agent4);
  \end{tikzpicture}
  \label{subfig:isolated}
}

\caption{Experimental configurations of the agent organisations with optimal resource allocation (see table \ref{tab:optimal_r}).}
\label{fig:network_configurations}
\end{figure}


\section{Results}
\label{sec:results}

In this section we present results obtained using three models: DTMC, MDP, and STPG. Table \ref{tab:model_sizes} compares model construction information for different sizes of fully connected agent organisations \footnote{We chose fully connected agent organisation because it produces largest models.}: model size in terms of number of states and transitions and construction time. All of the experiments were performed on a 2.8GHz Intel Core 2 PC, 4Gb of RAM running Fedora Core 10 operating system. It is interesting to see that nondeterministic model has a relatively smaller state space because agent choices do not have to be permuted at model construction stage. However, the model checking is generally more time consuming for MDPs and STPGs than for DTMCs.


\begin{table}
\centering
\subfloat[DTMC]{
\begin{tabular}{ | l | l | l | l |}
    \hline
    Agents & States & Transitions & Const. Time \\ \hline
    2 & 1865 & 2256 & 0.112s  \\ \hline
    3 & 17041 & 20904 & 0.311s \\ \hline
    4 & 184753 & 226736 & 3.347s \\ \hline
    5 & 2366305 & 2893536 & 74.36s \\ \hline
    6 & 35058241 & 42638400 & 2916.235s \\ \hline
\end{tabular}
}
\subfloat[MDP and STPG]{
 \begin{tabular}{ | l | l | l | l |}
    \hline
    Agents & States & Transitions & Const. Time \\ \hline
    2 & 1405 & 1846 & 0.045s  \\ \hline
    3 & 9721 & 12474 & 0.167s \\ \hline
    4 & 76865 & 96664 & 1.109s \\ \hline
    5 & 731233 & 907992 & 5.0536s \\ \hline
    6 & 8155873 & 10040112 & 29.74s \\ \hline
\end{tabular}
}

\caption{Model comparison for different number of agents in a fully connected agent organisation and different models for offline version of the Algorithm \ref{alg:main_process}.}
\label{tab:model_sizes}
\end{table}

As mentioned in Section \ref{subsec:exp_set}, for each topology from Figure \ref{fig:network_configurations} we obtained the optimal resource allocations using PCTL model checking on DTMC model of the offline version of the algorithm (see Algorithms \ref{alg:main_process} and \ref{alg:join_team_org}). The following PCTL formulae were used to compute the expected rewards of the agent organisation under a particular resource allocation:
\begin{itemize}
 \item ${\tt R}_{\{``W_{\{1,2\}}(O)''\}=?}[\,\Diamond\,\mathit{finished}\,]$ -
``what is the expected total reward  $W_1$/$W_2$ of an agent organisation when execution terminates?'',
\end{itemize}
After obtaining the expected rewards for all possible resource allocations we then selected the ones which have the highest expected reward. The results are summarised in Table \ref{tab:optimal_r}.


\begin{table}[H]
 \centering
 \begin{tabular}{ | l | l | l | l |}
    \hline
    Organisation $O$ & Additional constraints & Example $\LD R_{a_1}R_{a_2}R_{a_3}R_{a_4}R_{a_5}\RD$ \\ \hline
    $O_{fc}$ & - & $R_A=\LD \{r_1\}\{r_1\}\{r_2\}\{r_2\}\{r_3\}\RD$  \\ \hline
    $O_r$ & $R_{a_1}\neq R_{a_5} \wedge \forall i < 5 . R_{a_i} \neq  R_{a_{i+1}} $ & $R_A=\LD \{r_1\}\{r_2\}\{r_1\}\{r_2\}\{r_3\}\RD$  \\ \hline
    $O_s$ & $R_{a_1}=\{r\} \wedge \forall i > 1 . r \notin R_{a_i} $  & $R_A=\LD \{r_1\}\{r_2\}\{r_2\}\{r_3\}\{r_3\}\RD$  \\ \hline
    $O_{ia}$ & $R_{a_5}=\{r\} \wedge \exists i < 4 . r \in R_{a_i} $ & $R_A=\LD \{r_1\}\{r_2\}\{r_3\}\{r_3\}\{r_2\}\RD$  \\ \hline
\end{tabular}
\caption{Optimal resource allocations ($R_A$) for agent organisations from figure \ref{fig:network_configurations} with respect to rewards defined in equations \ref{eq:w1organisation} and \ref{eq:w2organisation}. All allocations satisfy the following constraint $\forall i. |R_{a_i}|=1 \wedge   \forall i.1 \le|\{ R_{a_j} : r_i \in R_{a_j} (1 \le j \le 5 )\}|\le 2$.}
\label{tab:optimal_r}
\end{table}

Please note that the resource allocations from Table \ref{tab:optimal_r}'s example column have been chosen for all future experiments and are shown in Figure \ref{fig:network_configurations}. we have decided to fix resource allocations in this way as it allows us to show how model-checking techniques can be used to improve performance of the algorithm by synthesising strategies (see discussion of MDP results in Section \ref{subsec:MDP}).
\clearpage
\subsection{DTMC}
\label{subsection:DTMC}

In this section we present results of model checking DTMC model of Algorithm \ref{alg:join_team_org}. We consider four experimental agent organisations from Figure \ref{fig:network_configurations}, as well as offline and online versions of the algorithm (see Figure \ref{alg:main_process} for details). To discuss the results, we compare overall performance of agent organisations and consider disparity between worst and best individually performing agents within them. Then we move on to discuss the effects of network topologies and offline vs online versions of the algorithm on the aforementioned characteristics. Finally, we discuss several interpretations of reward structures and argue why it is important to consider different ones to get better understanding of the system.

\begin{table} [H]
\subfloat[Offline. $W_1$ reward structure.]{
 \begin{tabular}{ | l | l | l | l | l |}
    \hline
    $O$ & $W_1(O)$ & $\min_{a \in A}W_1(a)$ & $\max_{a \in A}W_1(a)$ \\ \hline
    $O_{fc}$ & 2.54906 & 0.44958 & 0.75073   \\ \hline
    $O_r$  & 2.30359 & 0.35494 & 0.63985 \\ \hline
    $O_s$  & 1.87278 & 0.28677 & 0.72568  \\ \hline
    $O_{ia}$  & 2.38529 & 0.28867 & 0.68769  \\ \hline
\end{tabular}
}
\subfloat[Offline. $W_2$ reward structure.]{
 \begin{tabular}{ | l | l | l | l |}
    \hline
    $O$ & $W_2(O)$ & $\min_{a \in A}W_2(a)$ & $\max_{a \in A}W_2(a)$ \\ \hline
    $O_{fc}$ & 1.49125 & 0.26721 & 0.42238 \\ \hline
    $O_r$ & 1.42923 & 0.23531 & 0.38625 \\ \hline
    $O_s$ & 1.16649 & 0.18582 & 0.42321  \\ \hline
    $O_{ia}$ & 1.43599 & 0.20621 & 0.39907  \\ \hline
\end{tabular}
}

\subfloat[Online. $W_1$ reward structure.]{
 \begin{tabular}{ | l | l | l | l | l |}
    \hline
    $O$ & $W_1(O)$ & $\min_{a \in A}W_1(a)$ & $\max_{a \in A}W_1(a)$ \\ \hline
    $O_{fc}$ & 3.53645 & 0.64101 & 0.97239   \\ \hline
    $O_r$  & 3.48638 & 0.55089 & 0.91190 \\ \hline
    $O_s$  & 2.52500 & 0.41934 & 0.84761  \\ \hline
    $O_{ia}$  & 3.37359 & 0.41186 & 0.93601  \\ \hline
\end{tabular}
}
\subfloat[Online. $W_2$ reward structure.]{
 \begin{tabular}{ | l | l | l | l |}
    \hline
    $O$ & $W_2(O)$ & $\min_{a \in A}W_2(a)$ & $\max_{a \in A}W_2(a)$ \\ \hline
    $O_{fc}$ & 1.29743 & 0.24247 & 0.32657 \\ \hline
    $O_r$ & 1.31882 & 0.23157 & 0.31297 \\ \hline
    $O_s$ & 0.94404 & 0.16060 & 0.30158  \\ \hline
    $O_{ia}$ & 1.25560 & 0.17970 & 0.31990  \\ \hline
\end{tabular}
}
\caption{Model checking results for agent organisations from figure \ref{fig:network_configurations} with optimal resource allocations from table \ref{tab:optimal_r} for offline and online versions of the Algorithm \ref{alg:main_process}. Tables also show largest and smallest individual agent rewards. }
\label{tab:dtmc_results}
\end{table}

Table \ref{tab:dtmc_results} shows the results obtained for the expected rewards of agent organisations in different settings, namely, using $W_1$ and $W_2$ reward structures (see Equations \ref{eq:w1organisation} and \ref{eq:w2organisation} for organisations and Equations \ref{eq:w1agent} and \ref{eq:w2agent} for individual agents), and offline and online versions of the Algorithm \ref{alg:join_team_org}. The following PCTL formulae were used to obtain the results in PRISM:
\begin{itemize}
 \item ${\tt R}_{\{``W_{\{1,2\}}(O)''\}=?}[\,\Diamond\,\mathit{finished}\,]$ -
``what is the expected total reward $W_1$/$W_2$?'',
 \item ${\tt R}_{\{``W_{\{1,2\}}(O)''\}=?}[\,\Diamond\,\mathit{finished} \wedge a_i \in \bar{M} \,]$ -
``what is the expected reward $W_1$/$W_2$ for agent $a_i$?''
\end{itemize}

As can be seen in Table \ref{tab:dtmc_results} agents organised in start topology $O_s$ have the worst expected rewards in all settings, also it has largest disparity between worst and best performing individual agents. Both of these characteristics are unsurprising because $a_1$ which is placed in the middle is most likely to be in a winning team, whereas all the others do not have any choice but to join a team with $a_1$. Organisation $O_{ia}$ which has one isolated agent shows smaller differences between worst and best performing agents, but this is only because the performance of ``best'' agents, whereas the ``worst'' agent's performance is very close to the $O_s$'s.

Figure \ref{fig:bar_chart_dtmc} compares total rewards of all organisation in offline and online settings. It can be seen that a fully connected organisation $O_{fc}$ has the best overall performance in all but the online version using $W_2$ reward structure, where it is outperformed by organisation $O_r$ which arranged in a ring. It is interesting to see that a more significant difference between $O_{fc}$ and $O_{ia}$ only emerges when moving into algorithm's online setting, this shows that having one agent which is isolated does not affect the ability of the organisation to efficiently respond to a generated tasks, but impacts its chances to organise before tasks are generated. And this where the advantages $O_r$ emerge, in online setting it not starts outperfoming $O_{ia}$ in respect with both overall performance and disparity, but gets very close to the performance of $O_{fc}$ using $W_1$ reward structure and produces a better total $W_2$ reward, while keeping disparity levels close.

\begin{figure}[H]
\subfloat[Reward $W_1(O)$.]{
  \includegraphics[clip=true, trim=75 570 305 90, scale=0.77]{images/w1_dtmc}
}
\subfloat[Reward $W_2(O)$.]{
  \includegraphics[clip=true, trim=70 579 300 90, scale=0.77]{images/w2_dtmc}
}
\caption{Expected rewards for agent organisations when using online and offline (see Algorithm \ref{alg:main_process}) versions of the Algorithm \ref{alg:join_team_org}.}
\label{fig:bar_chart_dtmc}
\end{figure}
\begin{table}[H]
 \centering
\subfloat[Offline]{
 \begin{tabular}{ | l | l | l | l |}
    \hline
    $O$ & $T_1$ compl. & $T_2$ compl. & $T_1$ and $T_2$ compl. \\ \hline
    $O_{fc}$ & 0.74562 & 0.74562 & 0.49596  \\ \hline
    $O_r$ & 0.71461 & 0.71461 & 0.47062 \\ \hline
    $O_s$ & 0.58324 & 0.58324 & 0.23639 \\ \hline
    $O_{ia}$ & 0.71799 & 0.71799 & 0.44839 \\ \hline
\end{tabular}
}
\subfloat[Online]{
 \begin{tabular}{ | l | l | l | l |}
    \hline
    $O$ & $T_1$ compl. & $T_2$ compl. & $T_1$ and $T_2$ compl. \\ \hline
    $O_{fc}$ & 0.64871 & 0.64871 & 0.31320  \\ \hline
    $O_r$ & 0.65941 & 0.65941 & 0.36712 \\ \hline
    $O_s$ & 0.47202 & 0.47202 & 0.07465 \\ \hline
    $O_{ia}$ & 0.62780 & 0.62780 & 0.29270 \\ \hline
\end{tabular}
}
\caption{Task completion probabilities for optimal agent organisations using Algorithm \ref{alg:join_team_org}'s offline and online versions (see Algorithm \ref{alg:main_process}).}
\label{tab:task_compl_dtmc}
\end{table}

Attentive reader would have noticed that the online version produces greater expected $W_1$, but smaller expected $W_2$ rewards than the offline one. This is an interesting observation because it shows that in an online version algorithm organises agents into teams which increases the expectation for more each agents to be in a successful team, but this effect decreases the expected total number of tasks completed by the organisation. This is confirmed by the probabilities of task completions of offline and online versions of Algorithm \ref{alg:join_team_org} that are summarised in Table \ref{tab:task_compl_dtmc}. The following PCTL formulae were used to find the probabilities using PRISM:
\begin{itemize}
 \item ${\tt P}_{=?}[\,\Diamond\,T_{\{1,2\}} \mathit{\ completed}\,]$ -
``what is the expected probability to complete  task $T_1$/$T_2$?'',
 \item ${\tt P}_{=?}[\,\Diamond\,T_{1} \wedge T_2 \mathit{\ completed}\,]$ -
``what is the expected probability to complete  both tasks $T_1$ and $T_2$?'',
\end{itemize}

Let us note that the strategy of an agent does not change when switching from offline to online setting, but it is the same strategy and the removal of constraint that the agent can only join a team for a task that it has the required skill for (which is obviously not known before the task is advertised) that results more agents joining teams. In the next section we will explore how the performance can be improved by changing the strategy of the agents so that they collaborate to optimise performance of the organisation.


\clearpage

\subsection{MDP}
\label{subsec:MDP}

\aistis{We need to add PCTLs used for the generation of results}

In this section we present the analysis of Algorithm \ref{alg:join_team_nondet} which is a modified version of the original Algorithm \ref{alg:join_team_org}, but changed so that actions taken by the agents remain the same but they have full control over what strategies to apply. Since this introduced nondeterminism into the algorithm, natural model for such system becomes Markov Decision Process. Using PRISM model checker we find the maximum expected rewards and task completion probabilities for all agent organisations and compare the results with a specific strategy which was Algorithm \ref{alg:join_team_org}. Due to the size limitations we do not present the actual strategies in this paper\footnote{The instructions on how to generate optimal strategies for MDPs can be found in ...}.

\begin{table}
\centering
\subfloat[Offline]{
 \begin{tabular}{ | l | l | l | l |}
    \hline
    $O$ & $T_1$ compl. & $T_2$ compl. & $T_1$ and $T_2$ compl. \\ \hline
    $O_{fc}$ & 1.0 & 1.0 & 0.67346  \\ \hline
    $O_r$ & 1.0 & 1.0 & 0.67346  \\ \hline
    $O_s$ & 0.82857 & 0.82857 & 0.39183 \\ \hline
    $O_{ia}$ & 1.0 & 1.0 & 0.67346 \\ \hline
\end{tabular}
}
\subfloat[Online]{
 \begin{tabular}{ | l | l | l | l |}
    \hline
    $O$ & $T_1$ compl. & $T_2$ compl. & $T_1$ and $T_2$ compl. \\ \hline
    $O_{fc}$ & 1.0 & 1.0 & 0.42857  \\ \hline
    $O_r$ & 1.0 & 1.0 & 0.42857 \\ \hline
    $O_s$ & 0.88571 & 0.88571 & 0.12653 \\ \hline
    $O_{ia}$ & 1.0 & 1.0 & 0.42857 \\ \hline
\end{tabular}
}
\caption{Maximum task completion probabilities for optimal agent organisations using algorithm \ref{alg:join_team_nondet}'s online and offline versions (see algorithm \ref{alg:main_process}). Results obtained by model checking PCTL formulae on MDP model.}
\label{tab:mdp_probs}
\end{table}

In Table \ref{tab:mdp_probs} we can see the maximum expected organisation rewards that can achieved by all agents collaborating. All organisations except $O_s$ can make sure that at least one task is completed with probability $1.0$, no matter what the scheduling is.


\begin{table}
\centering
\subfloat[Offline]{
 \begin{tabular}{ | l | l | l | l |}
    \hline
    $O$ & Max $W_1(O)$ & Max $W_2(O)$ \\ \hline
    $O_{fc}$ & 2.89795 & 1.67346   \\ \hline
    $O_r$ & 2.89795 & 1.67346  \\ \hline
    $O_s$ & 2.20816 & 1.35918  \\ \hline
    $O_{ia}$ & 2.89795 & 1.67346  \\ \hline
\end{tabular}
}
\subfloat[Online]{
 \begin{tabular}{ | l | l | l | l |}
    \hline
    $O$ & Max $W_1(O)$ & Max $W_2(O)$ \\ \hline
    $O_{fc}$ & 3.85714 & 1.42857   \\ \hline
    $O_r$ & 3.85714 & 1.42857  \\ \hline
    $O_s$ & 2.71428 & 1.02857  \\ \hline
    $O_{ia}$ & 3.85714 & 1.42857  \\ \hline
\end{tabular}
}
\caption{Maximal rewards that can be achieved by agent organisations from figure \ref{fig:network_configurations} using algorithm \ref{alg:join_team_nondet}'s offline and online versions defined in algorithm \ref{alg:main_process}.}
\label{tab:mdp_rewards}
\end{table}

In Table \ref{tab:mdp_rewards} we see the maximum expected organisation rewards that can achieved by all agents collaborating. It is not very difficult to see that $O_{fc}$, $O_r$ and $O_{ia}$ have the same maximum reward no matter W1/W2 reward or online/offline version is taken, which outperforms the star organization $O_s$ in all circumstances.

Figure \ref{fig:bar_chart_mdp} compares maximum expected rewards with a specific strategy used in the original algorithm, for all agent organisations in both offline and online settings.
 %
%
%
%It can be seen that a fully connected organisation $O_{fc}$ has the best overall performance in all but the online version using $W_2$ reward structure, where it is outperformed by organisation $O_r$ which arranged in a ring. It is interesting to see that a more significant difference between $O_{fc}$ and $O_{ia}$ only emerges when moving into algorithm's online setting, this shows that having one agent which is isolated does not affect the ability of the organisation to effciently respond to a generated tasks, but impacts its chances to organise before tasks are generated. And this where the advantages $O_r$ emerge, in online setting it not starts outperfoming $O_{ia}$ in respect with both overall performance and disparity, but gets very close to the performance of $O_{fc}$ using $W_1$ reward structure and produces a better total $W_2$ reward, while keeping disparity levels close.

\begin{figure}[H] \label{fig:bar_chart_mdp}
\subfloat[Offline. Reward $W_1(O)$.]{
\includegraphics[clip=true, trim=60 580 320 90, scale=0.77]{images/w1_mdp_offline}
}
\subfloat[Online. Reward $W_1(O)$.]{
\includegraphics[clip=true, trim=57 580 320 90, scale=0.77]{images/w1_mdp_online}
}

\subfloat[Offline. Reward $W_2(O)$.]{
\includegraphics[clip=true, trim=94.5 570 280 100, scale=0.77]{images/w2_mdp_offline}
}
\subfloat[Online. Reward $W_2(O)$.]{
\includegraphics[clip=true, trim=63 581.5 320 80, scale=0.77]{images/w2_mdp_online}
}
\caption{Bar charts compare offline and online versions of Algorithm \ref{alg:join_team_org} analysed as DTMC with the best-case performance of Algorithm \ref{alg:join_team_nondet} analysed as MDP. }
\end{figure}


%\clearpage

\subsection{STPG}

In this section we focus on the analysis of Algorithm \ref{alg:join_team_nondet}, but this time we consider a distinction of agents with cooperative and hostile ones, which is  naturally modelled as a STPG. We are mainly interested in finding the optimal coalition in the sense that it has the largest probability to accomplish tasks. A coalition is a subset of agents who cooperate against the rest of agents in the organization. To this aim, we consider a slight extension of
PCTL formulae with agent coalitions:

\aistis{PCTL notation needs to be synced with the preliminaries section.}

\begin{itemize}
 \item $[A]{\tt P}_{=?}[\,\Diamond\,T_{\{1,2\}} \mathit{\ completed}\,]$ -
``what is the expected probability to complete  task $T_1$/$T_2$?'',
 \item $[A]{\tt P}_{=?}[\,\Diamond\,T_{1} \wedge T_2 \mathit{\ completed}\,]$ -
``what is the expected probability to complete  both tasks $T_1$ and $T_2$?'',
\end{itemize}

%Let us note that the strategy of an agent does not change when switching from offline to online setting, but it is the same strategy and the removal of constraint that the agent can only join a team for a task that it has the required skill for (which is obviously not known before the task is advertised) that results more agents joining teams. In the next section we will explore how the performance can be improved by changing the strategy of the agents so that they collaborate to optimise performance of the organisation.
%
We enumerate all the possible coalitions with different sizes and in each case, PRISM model checker returns task completion probabilities (one task or both tasks). This is done for both online and offline versions of the algorithm. Table \ref{tab:optimal_coalitions} shows the optimal coalitions with different sizes. Table \ref{tab:optimal_coalitions_1task} shows that value for completion of one task, whereas Table \ref{tab:optimal_coalitions_1task} shows the value for two tasks. 
\begin{table}
 \centering
 \begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    $O$ & 1& 2 & 3 & 4 & 5 \\ \hline
    $O_{fc}$ & $\LD a_1 \RD$ & $\LD a_1, a_3 \RD$ & $\LD a_1, a_3, a_5 \RD$ & $\LD a_1, a_2, a_3, a_5 \RD$  & $\LD a_1, a_2, a_3, a_4, a_5 \RD$ \\ \hline
    $O_r$ & $\LD a_1 \RD$  & $\LD a_2, a_3 \RD$ & $\LD a_1, a_4, a_5 \RD$ & $\LD a_1, a_2, a_4, a_5 \RD$  & $\LD a_1, a_2, a_3, a_4, a_5 \RD$ \\ \hline
    $O_s$ & $\LD a_1 \RD$  & $\LD a_1, a_2 \RD$ & $\LD a_1, a_2, a_4 \RD$ & $\LD a_1, a_2, a_3, a_4 \RD$  & $\LD a_1, a_2, a_3, a_4, a_5 \RD$ \\ \hline
    $O_{ia}$ & $\LD a_1 \RD$  & $\LD a_1, a_4 \RD$ & $\LD a_1, a_2, a_4 \RD$ & $\LD a_1, a_2, a_4, a_5 \RD$  & $\LD a_1, a_2, a_3, a_4, a_5 \RD$ \\ \hline
\end{tabular}
\caption{Optimal coalitions of sizes 2, 3, and 4 for agent organisations from figure \ref{fig:network_configurations} for both online and offline versions.}
\label{tab:optimal_coalitions}
\end{table}

\begin{table}
 \centering
\subfloat[Offline]{
  \begin{tabular}{ | l | l | l | l | l | l |}
      \hline
      $O$ & 1 & 2 & 3 & 4 & 5 \\ \hline
      $O_{fc}$ & 0.14285 & 0.42857 & 1.0 & 1.0 & 1.0  \\ \hline
      $O_r$ & 0.14285 & 0.42857 & 0.68333 &  0.92619 & 1.0 \\ \hline
      $O_s$ & 0.14285 & 0.42857 & 0.79761 & 0.81904 & 0.82857\\ \hline
      $O_{ia}$ & 0.14285 & 0.42857 & 0.93452 & 1.0 & 1.0\\ \hline
  \end{tabular}
}
\subfloat[Online]{
  \begin{tabular}{ | l | l | l | l | l | l |}
      \hline
      $O$ & 1 & 2 & 3 & 4 & 5 \\ \hline
      $O_{fc}$ & 0.14285 & 0.42857 & 1.0 & 1.0 & 1.0  \\ \hline
      $O_r$ & 0.14285 & 0.33333 & 0.68571 & 0.89523 & 1.0 \\ \hline
      $O_s$ & 0.14285 & 0.42857 & 0.76190 & 0.84761 & 0.88571 \\ \hline
      $O_{ia}$ & 0.14285 & 0.42857 & 0.92857 & 1.0 & 1.0\\ \hline
  \end{tabular}
}\label{tab:optimal_coalitions_1task}
\caption{Values for maximum probabilities to complete one task for coalitions from table \ref{tab:optimal_coalitions}.}
\end{table}


\begin{table}
 \centering
\subfloat[Offline]{
 \begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    $O$ & 1 & 2 & 3 & 4 & 5 \\ \hline
    $O_{fc}$ & 0.0 & 0.04081 & 0.24489 &  0.42857 & 0.67346  \\ \hline
    $O_r$ & 0.0 &  0.04081 & 0.17687 & 0.40748 & 0.67346\\ \hline
    $O_s$ & 0.0 & 0.04081 & 0.20408 & 0.29523 & 0.39183 \\ \hline
    $O_{ia}$ & 0.0 & 0.04081 & 0.23129 & 0.41496 & 0.67346\\ \hline
\end{tabular}
}
\subfloat[Online]{
 \begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    $O$ & 1 & 2 & 3 & 4 & 5 \\ \hline
    $O_{fc}$ & 0.0 & 0.02040 & 0.06122 &  0.18367 & 0.42857  \\ \hline
    $O_r$ & 0.0 & 0.02040 & 0.06122 & 0.18367 & 0.42857\\ \hline
    $O_s$ & 0.0 & 0.02040 & 0.06122 & 0.12244 & 0.12653 \\ \hline
    $O_{ia}$ & 0.0 & 0.02040 & 0.06122 & 0.18231 & 0.42857\\ \hline
\end{tabular}
}
\label{tab:optimal_coalitions_2task}
\caption{Values for maximum probabilities to complete both tasks for coalitions from table \ref{tab:optimal_coalitions}.}
\end{table}

\clearpage

\section{Conclusions and Future Work}

In this paper we have applied probabilistic model checking techniques to analyze a team formation protocol protocols in multi-agent system, using the PRISM model checker. We analysed the organization performance of the original protocol by modelling as a DTMC. And we extended the algorithm by allowing agents to make decisions regarding what action to take, for which we provided MDP and STPG models. In these two cases, two performance measures were considered, i.e., the agent organization one and each agent's local one. By the MDP modelling, we could obtain the best performance
of running the team joining algorithm individually by each agent in the protocol, while by the STPG modelling, we extended such analysis to the competitive coalitional settings. In our experiments, each agent has only one resource. However, this can be easily extended to the setting that agents having multiple resources.

\paragraph{Future work.} Although we have conducted a rather comprehensive analysis of the team formation protocol, many interesting directions remain as ongoing and future work. Here we simply outline some of them:

\begin{itemize}
\item Verify ``dual" properties. For instance, rather than asking ``what is the expected reward'', we would like to be able to answer questions like ``what is the probability of reaching reward value $R$?''. This also applies to the strategy synthesis setting.

\item Explore parallel execution of the \textsc{JoinTeam} algorithm for multiple agents, as here we only considered sequential execution. %(i.e. interleaving steps of JoinTeam algorithm
    In particular, it would be interesting to investigate how this affects strategies of agents in both online and offline versions of the algorithm. %Also this raises many interesting scheduling problems, as worst and best case scheduling (of action interleaving) scenarios are unrealistic, the challenging question is to both develop realistic schedulers and perform model-checking efficiently.

 %\item Conducting experiments with agents having multiple resources, allowing agents to change teams until stable point is reached and check whether it can reached, what is the expected number of steps to reach the stable configuration, etc.

 \item Synthesize the optimal agent organisation given a set of tasks. % and their distributions.
       This problem turns to be an instance of the mechanism design problem in game theory, where the designer can have control over neighbourhood structure or agent resources or both. We plan to attack this question by probabilistic model checking techniques.
\end{itemize}

\bibliographystyle{plain}
\bibliography{refs}

\appendix
\section{Formal Definitions of Agent Organizations}
\begin{itemize}
 \item Fully connected (figure \ref{subfig:fully_connected}) $O_{fc}=\langle A, N_{fc}, R, R_{A_{fc}}  \rangle$ where
    \begin{itemize}
    \item $N_{fc}=\{\{a_i,a_j \}\ : 1 \le i,j \le 5, i\neq j \}$,
    \item $R_{A_{fc}}=\{R_{a_1}, R_{a_2}, R_{a_3}, R_{a_4}, R_{a_5}\}$ where $R_{a_1}=\{r_1\}$, $R_{a_2}=\{r_1\}$, $R_{a_3}=\{r_2\}$, $R_{a_4}=\{r_2\}$, and $R_{a_5}=\{r_3\}$.
    \end{itemize}
 \item Ring (figure \ref{subfig:ring}) $O_r=\langle A, N_r, R, R_{A_r}  \rangle$ where
    \begin{itemize}
    \item $N_r=\{\{a_i,a_j \}\ : 1 \le i,j \le 5, i\neq j, i\le 5 \implies j=i+1, i=5 \implies j=1 \}$,
    \item $R_{A_r}=\{R_{a_1}, R_{a_2}, R_{a_3}, R_{a_4}, R_{a_5}\}$ where $R_{a_1}=\{r_1\}$, $R_{a_2}=\{r_2\}$, $R_{a_3}=\{r_1\}$, $R_{a_4}=\{r_2\}$, and $R_{a_5}=\{r_3\}$.
    \end{itemize}
 \item Star (figure \ref{subfig:star}) $O_s=\langle A, N_s, R, R_{A_s}  \rangle$ where
    \begin{itemize}
    \item $N_s=\{\{a_i,a_j \}\ : 1 \le i,j \le 5, i\neq j, i=1 \}$,
    \item $R_{A_s}=\{R_{a_1}, R_{a_2}, R_{a_3}, R_{a_4}, R_{a_5}\}$ where $R_{a_1}=\{r_1\}$, $R_{a_2}=\{r_2\}$, $R_{a_3}=\{r_2\}$, $R_{a_4}=\{r_3\}$, and $R_{a_5}=\{r_3\}$.
    \end{itemize}
 \item Isolated agent (figure \ref{subfig:isolated}) $O_{ia}=\langle A, N_{ia}, R, R_{A_{ia}}  \rangle$ where
    \begin{itemize}
    \item $N_{ia}=\{\{a_i,a_j \}\ : 1 \le i,j \le 5, i\neq j, (i=5 \vee j=5) \implies (i=4 \vee j=4) \}$,
    \item $R_{A_{ia}}=\{R_{a_1}, R_{a_2}, R_{a_3}, R_{a_4}, R_{a_5}\}$ where $R_{a_1}=\{r_1\}$, $R_{a_2}=\{r_2\}$, $R_{a_3}=\{r_3\}$, $R_{a_4}=\{r_3\}$, and $R_{a_5}=\{r_2\}$.
    \end{itemize}
\end{itemize}

\appendix
\label{app:transitions}
\section{State Transition System}
\begin{figure}
  \centering
  \vspace{-50pt}
	\includegraphics[]{images/transition_system}
	\vspace{-35pt}
	\caption{State transition system of the model from Figure \ref{fig:prism_example} generated by PRISM.}
\end{figure}

\end{document} 